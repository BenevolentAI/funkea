{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aacdcb4a-d1a5-4d61-89d4-a60d7cb1ac6e",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "In this tutorial, we will cover some of the basics of `funkea`, and run through a few simple examples of how one can get various enrichment results from GWAS sumstats. In these examples, we will use the `Fisher` method for computing the enrichments, as it is simple and quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a1794-2244-493d-8c8e-8598ec404281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from funkea.core import data\n",
    "from funkea.implementations import Fisher\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7257c-52dd-41a4-a4c5-a9f5c9cf1ddc",
   "metadata": {},
   "source": [
    "Provide the filepath to your GWAS sumstats. Here we used `ieu-b7` from OpenGWAS, which is a Parkinson's study on a European population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd60129-d284-44f3-ae8e-6a13471ed1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SUMSTATS_PATH = \"data/sumstats.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ab071-a89a-4a12-b33d-16f779abc3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[2]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sumstats = spark.read.parquet(SUMSTATS_PATH)\n",
    "sumstats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913595c9-5223-4f87-8cf7-0dd3690182f2",
   "metadata": {},
   "source": [
    "Next, we instantiate the `AnnotationComponent` object. This is an abstraction layer on top of the genomic annotations data (which is provided in tabular format), such that annotations can be used interchangeably. Here, we used (a subset of) the KEGG dataset, where the annotations are the genes and the partitions are the KEGG pathways. The partition type is `HARD`, i.e. a gene is either in a pathway or not; there is no distribution. The `dataset` is provided as a filepath, but we could just as well passed in a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36e568-0828-4cea-810d-f37459c5b9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kegg = data.AnnotationComponent(\n",
    "    columns=data.AnnotationColumns(\n",
    "        annotation_id=\"gene_id\", partition_id=\"pathway_name\"\n",
    "    ),\n",
    "    partition_type=data.PartitionType.HARD,\n",
    "    dataset=\"data/kegg.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf15e82-b2b5-4cb0-931e-e0a791bdd0e1",
   "metadata": {},
   "source": [
    "Now, we instantiate the model, using the `default` configuration (more on this in the next section). We make sure to pass the annotation component to the default configuration, to overwrite the default annotation component (GTEx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4cdb9-a59a-47ca-8937-cc5fae52de66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Fisher.default(annotation=kegg)\n",
    "enrichments = model.transform(sumstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa5db82-29fa-43b6-8f4e-a3edf20af94d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enrichments.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb98ed6-b401-491d-a163-00c4443d46cb",
   "metadata": {},
   "source": [
    "## Composability\n",
    "\n",
    "We saw how we could easily run functional enrichment experiments on GWAS sumstats using default configurations. However, `funkea` also offers ways of exploring various parameter settings and pipeline compositions to create new enrichment workflows.\n",
    "\n",
    "But before we do so, let us consider some concepts `funkea` employs to make this possible. Every workflow implementation follows the schematic show below:\n",
    "\n",
    "![schematic](docs/source/_static/schematic.png)\n",
    "\n",
    "i.e. each workflow consists of (1) a data pipeline; and (2) an enrichment method. The former filters down the sumstats (`variant_selection`), creates loci from the remaining variants (`locus_definition`) and then finally associates these loci with annotations (`annotation`). The latter then takes the loci (including their annotations) and computes the study-wide enrichments for each annotation partition, and its respective significance.\n",
    "\n",
    "In the following example, we will run the same enrichment experiment as above, but with some small modifications (purely for demonstration purposes):\n",
    "\n",
    "1. We reduce the $p$-value threshold and remove the dropping of ambiguous variants from the `variant_selector`.\n",
    "2. We replace a simple locus-annotation overlap with an overlap of an extended locus, i.e. we expand by $10,000$ base pairs into either direction.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> While \"variant_selection\" transforms are both idempotent and commutative, \"locus_definition\" transforms are not. That means, the order in which they appear matters and some will assume that others have come before (e.g. \"Merge\" depends on \"Collect\").\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cca932-6f69-4216-a3ee-040e729b45cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from funkea.components import locus_definition as ld\n",
    "from funkea.components import variant_selection as vs\n",
    "from funkea.implementations import fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5477b24-5bad-4384-bbd2-38bc7aedcbe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Fisher(\n",
    "    pipeline=fisher.Pipeline(\n",
    "        ld.Compose(\n",
    "            ld.Expand(extension=(10_000, 10_000)),\n",
    "            ld.Overlap(),\n",
    "            annotation=kegg\n",
    "        ),\n",
    "        variant_selector=vs.Compose(\n",
    "            vs.AssociationThreshold(\n",
    "                threshold=5e-10\n",
    "            ),\n",
    "            vs.DropHLA(),\n",
    "            vs.DropIndel(),\n",
    "        )\n",
    "    ),\n",
    "    method=fisher.Method()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cbd7eb-d567-4a87-9943-df3a29a6c492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.transform(sumstats).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
